---
output: github_document
editor_options: 
  chunk_output_type: console
bibliography: "inst/metapower.bib"
csl: "inst/apa.csl"
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# metapower <img src = 'man/figures/logo.png' align = "right" height = "180" />

<!-- badges: start -->
<!-- badges: end -->

The primary goal of metapower is to compute statistical power for meta-analyses. Currently, metapower is designed to compute statistical power for the following under fixed- and random-effects models:

1. Mean effect size difference between groups (e.g., Cohen's *d*)
2. Test of homogeneity for between-study variance
3. Categorical moderator analyses of the mean effect size

All mathematical calculations are derived from @hedges2004, @bornstein2009, and @pigott2012.

## Installation

You can install the released version of metapower from [CRAN](https://CRAN.R-project.org) with:

``` r
install.packages("metapower")
```

And the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("jasonwgriffin/metapower")
```

# Example 1: Computing power to detect mean difference effect size

Suppose that we plan to meta-analyze all published findings to compute a summary effect size estimate for the group difference between typically developing individuals and individuals with autism on a measure of face recognition ability. In order to plan the study accordingly, we must choose plausible values for the following:

1. Expected effect size
2. Expected sample size per group
3. Expected number of studies
4. Expected degree of heterogeneity (only for Random-effects model)

...*for our meta-analysis of face recognition deficits in autism*

1. We expect that face recognition deficits in ASD are small (Cohen's d = 0.25)
2. Sample sizes in autism research are generally small. We expect the average group size to be 20.
3. Face recognition is frequently studied in autism; therefore, we expect to find 50 studies.
4. Autism is notoriously heterogeneous. We expect large heterogeneity between-studies.

To do this with `metapower`, we use the core function `mpower()`

```{r}
library(metapower)
my_power <- mpower(effect_size = .25, 
                   sample_size = 20, 
                   k = 30, 
                   hg = "large", 
                   es_type = "d",
                   model = "random")

```

Note that we specify this a random-effects model (`model = "random`). 
For fixed-effects model, use `model = "fixed"`.

```{r}
print(my_power)
```

The first part of the output shows the expected input values, where the main results are shown in the bottom portion, mainly, `Estimated Power`. Under this set of values, our power to detect a mean difference is `r round(my_power$power*100,2)`%. Furthermore, we can look at the power to detect heterogeneity among included studies by examining the `Estimated Power for Test of Homogeneity` output, which for this set of values is `r round(my_power$homo_test*100,2)`%

To visualize the power curve for these set of input parameters, use `power_plot()` to generate a `ggobject` that is modifiable and by default, shows 10x as many studies as the user inputs. 

```{r fig.height=4, fig.width=7, dpi = 300}
power_plot(my_power)
```

For users wanting more flexibility in visualization, the `mpower` object contains a dataframe `$df` containing all data populating the `ggobject`,

```{r}
str(my_power$df)
```

## Example 2: Power analysis for moderation analysis (categorical variables)

Although researchers are primarily interested in conducting meta-analysis to quantify the main effect of a specific phenomenon, It is very common to evaluate the moderation of this overall effect based on a number of study- and/or sample-related characteristics such as task paradigm or age group (e.g., children, adolescents, adults). To compute the statistical power for the detection of categorical moderators, we use the function `mod_power()` with a few additional arguments, mainly:

1. Expected number of groups (`n_groups`):
2. Expected effect size of each group`effect_sizes`:

...*for our meta-analysis of face recognition deficits in autism*

We may expect that face recognition tasks have larger effect sizes then face perception tasks; therefore, we specify 2 groups and their respective expected effect sizes:
1. `n_groups` = 2
2. `effect_sizes` = c(.2,.5)


```{r}
my_moderation <- mod_power(n_groups = 2, 
                           effect_sizes = c(.2,.5), 
                           es_type = "d",
                           sample_size = 20,
                           k = 20,
                           hg = "large",
                           model = "random")
```

```{r}
print(my_moderation)
```

Given, this set of expected values, we have `r round(my_moderation$power_b*100,2)`% to detect between-group differences between face perception and face recognition deficits in ASD based on these set of expectations.

## References

<div id="refs"></div>

## Issues

If you encounter a clear bug, please file a minimal reproducible example on [github](https://github.com/jasonwgriffin/metapower/issues).



